{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "Description:\n",
        "\n",
        "Running the following scripts will collect our VisDrone data and load it into\n",
        "the structure outlined below:\n",
        "\n",
        "VisDrone/\n",
        "├── VisDrone2019-DET-test-dev/\n",
        "│   ├── annotations/\n",
        "│   └── images/\n",
        "├── VisDrone2019-DET-train/\n",
        "│   ├── annotations/\n",
        "│   └── images/\n",
        "├── VisDrone2019-DET-val/\n",
        "│   ├── annotations/\n",
        "│   └── images/\n",
        "├── VisDrone2019-DET-test-dev.zip\n",
        "├── VisDrone2019-DET-train.zip\n",
        "└── VisDrone2019-DET-val.zip\n",
        "\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "jQ_Fkq1Ee8XW",
        "outputId": "7da09e3b-5484-44ea-8710-c6728db8373f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nDescription:\\n\\nRunning the following scripts will collect our VisDrone data and load it into\\nthe structure outlined below, primarily utilizing gdown:\\n\\nVisDrone/\\n├── VisDrone2019-DET-test-dev/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-train/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-val/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-test-dev.zip\\n├── VisDrone2019-DET-train.zip\\n└── VisDrone2019-DET-val.zip\\n\\nWe then perform several high-level checks of paths and contents to confirm the\\nexpected structure above is present in the environment.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "57VpO4LJrXe1",
        "outputId": "72fd9290-728a-4cbc-b568-3d9e6ec79c9a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports (for version, see requirements.txt)\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "from torchvision.ops import box_iou, MultiScaleRoIAlign\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "h5ZgmXXziOda"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Parent Directory\n",
        "dataset_dir = \"/content/VisDrone\"\n",
        "os.makedirs(dataset_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "x2T0zOHUu2Hf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING DATASET\n",
        "# Download the file from Dropbox\n",
        "!wget -O VisDrone/VisDrone2019-DET-test-dev.zip \"https://www.dropbox.com/scl/fi/yuim21nvv96pdmetf43mu/VisDrone2019-DET-test-dev.zip?rlkey=m4htjb3wjdvjukshn3inh9s7d&st=ajdims0w&dl=1\"\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q VisDrone/VisDrone2019-DET-test-dev.zip -d VisDrone/VisDrone2019-DET-test-dev/"
      ],
      "metadata": {
        "id": "uDM17y7iIzX0",
        "outputId": "247fd124-9562-48d4-e872-8ac71f94ce61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 20:52:47--  https://www.dropbox.com/scl/fi/yuim21nvv96pdmetf43mu/VisDrone2019-DET-test-dev.zip?rlkey=m4htjb3wjdvjukshn3inh9s7d&st=ajdims0w&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com/cd/0/inline/CoqdrknQ8GNeoXAjOO0WHErov8yB921c8f6MS9sa_s_WixgskijNfSL0-HtNd3s8k9BAb7ekYY8-GqkOTIESoGOw6a7QvSoEtLWzDHadPt2EqNfdS8jm6Ej2USMFKHtRr7xFRQXcAvZ_9PCXXWNNyo_V/file?dl=1# [following]\n",
            "--2025-04-27 20:52:48--  https://uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com/cd/0/inline/CoqdrknQ8GNeoXAjOO0WHErov8yB921c8f6MS9sa_s_WixgskijNfSL0-HtNd3s8k9BAb7ekYY8-GqkOTIESoGOw6a7QvSoEtLWzDHadPt2EqNfdS8jm6Ej2USMFKHtRr7xFRQXcAvZ_9PCXXWNNyo_V/file?dl=1\n",
            "Resolving uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com (uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com (uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CoqpUr2hTfGK8mof37S_Ze86avKAYIMh1ASoPVVZG9czH9UhAyEkSuTucnx5uve3P7Q_CeyLVwTNYBpjV2Fy0HT7VYRzeQBTZYDMqVlmLlbpxg3_qAG6habGqEx_2SRzXdbNrUprXniaxLG4lJUkI35xyMo2udfydgx5gVJS0mBo-ulpCC8pph3KA6jqRYqyKh59DI2vGkLLvZtTokVdOMdXtmzDne158bAk030D3LWB46hteurw7t6wg6uTrlV_IW2lbF61CNfHNA5nkg9d1NKGENrbIKKXNNAEcwhgXuPDl8n0RbNfob9eH9Du87xtA9WquJnwNNWx0JfuajGmE7FkguE5M_LjwUdL1wGmmW1Q9MxB7IpW9O1ahvENGbvuuDw/file?dl=1 [following]\n",
            "--2025-04-27 20:52:49--  https://uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com/cd/0/inline2/CoqpUr2hTfGK8mof37S_Ze86avKAYIMh1ASoPVVZG9czH9UhAyEkSuTucnx5uve3P7Q_CeyLVwTNYBpjV2Fy0HT7VYRzeQBTZYDMqVlmLlbpxg3_qAG6habGqEx_2SRzXdbNrUprXniaxLG4lJUkI35xyMo2udfydgx5gVJS0mBo-ulpCC8pph3KA6jqRYqyKh59DI2vGkLLvZtTokVdOMdXtmzDne158bAk030D3LWB46hteurw7t6wg6uTrlV_IW2lbF61CNfHNA5nkg9d1NKGENrbIKKXNNAEcwhgXuPDl8n0RbNfob9eH9Du87xtA9WquJnwNNWx0JfuajGmE7FkguE5M_LjwUdL1wGmmW1Q9MxB7IpW9O1ahvENGbvuuDw/file?dl=1\n",
            "Reusing existing connection to uc59f939528ff4eddad4fa9901c3.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 311045829 (297M) [application/binary]\n",
            "Saving to: ‘VisDrone/VisDrone2019-DET-test-dev.zip’\n",
            "\n",
            "VisDrone/VisDrone20 100%[===================>] 296.64M  22.7MB/s    in 15s     \n",
            "\n",
            "2025-04-27 20:53:05 (19.8 MB/s) - ‘VisDrone/VisDrone2019-DET-test-dev.zip’ saved [311045829/311045829]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING DATASET\n",
        "# Download the file from Dropbox\n",
        "!wget -O VisDrone/VisDrone2019-DET-train.zip \"https://www.dropbox.com/scl/fi/xyjppciooyq0juffv0g6y/VisDrone2019-DET-train.zip?rlkey=y5hnuo2imr1we88xzvndmkzct&st=6exf6hz8&dl=1\"\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q VisDrone/VisDrone2019-DET-train.zip -d VisDrone/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ixa-O-vKkV6",
        "outputId": "848a92e8-52f1-4a46-f6f6-c029ac1c29f6",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 20:53:09--  https://www.dropbox.com/scl/fi/xyjppciooyq0juffv0g6y/VisDrone2019-DET-train.zip?rlkey=y5hnuo2imr1we88xzvndmkzct&st=6exf6hz8&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com/cd/0/inline/CopqdumQfGld-VN4ua3r_uTJkm9kHzKmJTxHOWX7QnfjIwRyU_1GV7FrfFukXqk0z5hbnvoOUuex2qTmbGqJbRTpiZLkzcwf_cIJxGKYLq1MgaZXLFt_NCVf8u8YDIQvnJl65F9QZEwGea6kI_zpzV9t/file?dl=1# [following]\n",
            "--2025-04-27 20:53:10--  https://uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com/cd/0/inline/CopqdumQfGld-VN4ua3r_uTJkm9kHzKmJTxHOWX7QnfjIwRyU_1GV7FrfFukXqk0z5hbnvoOUuex2qTmbGqJbRTpiZLkzcwf_cIJxGKYLq1MgaZXLFt_NCVf8u8YDIQvnJl65F9QZEwGea6kI_zpzV9t/file?dl=1\n",
            "Resolving uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com (uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com (uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cor1rA_URg7NbeJlWf3E6-eozq4413SfGg1bbM5885kbwQ22flpFVOtdeL1UmhpYO-KxGGZ1CWhozllSxgDSFy7RQiMHvnCoFbKJYQV_nU0K3oeXRCJ3Ww9tcbEOC-t7hTgNWAKzB5MA9aoss11VKKivebcZBKbdr95BA07qltRJj-1HF0XzdNTcGqfEZg4RRIMslpCkYqoQ3u0DGkzEECQk2RybMjQ7_yEczsgfIroWHWqN0KQ2AcCPj6i7tDywihktEFWUTyw_w-KiFoYhsBTVtrKx0cmT2yvlmgUBWedjEO7jsk46I6UpSHIWJcC1Jd8Tr6DVF_Rrp6EC6kwPB_cGQ79TSRyN24Si_x2GlTGMtGiGMY3tJyT_DIcCqgtKass/file?dl=1 [following]\n",
            "--2025-04-27 20:53:11--  https://uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com/cd/0/inline2/Cor1rA_URg7NbeJlWf3E6-eozq4413SfGg1bbM5885kbwQ22flpFVOtdeL1UmhpYO-KxGGZ1CWhozllSxgDSFy7RQiMHvnCoFbKJYQV_nU0K3oeXRCJ3Ww9tcbEOC-t7hTgNWAKzB5MA9aoss11VKKivebcZBKbdr95BA07qltRJj-1HF0XzdNTcGqfEZg4RRIMslpCkYqoQ3u0DGkzEECQk2RybMjQ7_yEczsgfIroWHWqN0KQ2AcCPj6i7tDywihktEFWUTyw_w-KiFoYhsBTVtrKx0cmT2yvlmgUBWedjEO7jsk46I6UpSHIWJcC1Jd8Tr6DVF_Rrp6EC6kwPB_cGQ79TSRyN24Si_x2GlTGMtGiGMY3tJyT_DIcCqgtKass/file?dl=1\n",
            "Reusing existing connection to uc3925d32f4559b8f685a5bed7b6.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1549875511 (1.4G) [application/binary]\n",
            "Saving to: ‘VisDrone/VisDrone2019-DET-train.zip’\n",
            "\n",
            "VisDrone/VisDrone20 100%[===================>]   1.44G  22.1MB/s    in 88s     \n",
            "\n",
            "2025-04-27 20:54:41 (16.7 MB/s) - ‘VisDrone/VisDrone2019-DET-train.zip’ saved [1549875511/1549875511]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VALIDATION DATASET\n",
        "# Download the file from Dropbox\n",
        "!wget -O VisDrone/VisDrone2019-DET-val.zip \"https://www.dropbox.com/scl/fi/op5lfc9g1eqjx0hmz5k66/VisDrone2019-DET-val.zip?rlkey=06rpa2gcfdzw1dc8vud39bypr&st=v1ndh3zz&dl=1\"\n",
        "\n",
        "# Unzip the file\n",
        "!unzip -q VisDrone/VisDrone2019-DET-val.zip -d VisDrone/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Km2b2_2Kkxt",
        "outputId": "0653f3d1-b5db-4b66-992a-6d09cba23166",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-27 20:59:34--  https://www.dropbox.com/scl/fi/op5lfc9g1eqjx0hmz5k66/VisDrone2019-DET-val.zip?rlkey=06rpa2gcfdzw1dc8vud39bypr&st=v1ndh3zz&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com/cd/0/inline/Cor9CKfnFhVlIkoeQmq7M1VrOyf13vYbklWTvRpJuqkQOLRQo08EgVlGcqfBqnreVMV4e5qT9SajQ6sC1IbEBlu_3_Axg07HLbe_w_MiczS77n3IgJ8N3f1wMLXsF6T6PlzoS1i5yAz5PhfJjJGXREOm/file?dl=1# [following]\n",
            "--2025-04-27 20:59:35--  https://uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com/cd/0/inline/Cor9CKfnFhVlIkoeQmq7M1VrOyf13vYbklWTvRpJuqkQOLRQo08EgVlGcqfBqnreVMV4e5qT9SajQ6sC1IbEBlu_3_Axg07HLbe_w_MiczS77n3IgJ8N3f1wMLXsF6T6PlzoS1i5yAz5PhfJjJGXREOm/file?dl=1\n",
            "Resolving uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com (uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com)... 162.125.66.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com (uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com)|162.125.66.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cop4jkGJSykNRYmHjE7nOERObd_QgiJKZ_2X57J3NROWkDLyNssiYiq92-jFY_gh4wtS9ZVQgronzVTg6XrFKv6SPLbmVnOIOV52Sk_5q5wbUogTDItxd9zgTE5brVcBi-hrXBIeSdkNeVnKKRiyC0pERavYETiCShqxC6e_W4iih0-LjodtZz3PGf6JEFNkk2OcSX1rvbvONYOc_tj8Kq34q1SwOFBMKVVoNPivPzZHWhTJoLdySDMxCDZy9NScDWYkq_tfbcB1usKFnCKBqSfEaDaER2QdNtTevCfbXH_NZmS2FAdvYad04jcQwjEgAMo48xSuI38RBwQ4Gjc6xBheYMm03ubzIb7yquaUuubRMfcdGTMXbB2VfpJVh9yP7Y8/file?dl=1 [following]\n",
            "--2025-04-27 20:59:36--  https://uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com/cd/0/inline2/Cop4jkGJSykNRYmHjE7nOERObd_QgiJKZ_2X57J3NROWkDLyNssiYiq92-jFY_gh4wtS9ZVQgronzVTg6XrFKv6SPLbmVnOIOV52Sk_5q5wbUogTDItxd9zgTE5brVcBi-hrXBIeSdkNeVnKKRiyC0pERavYETiCShqxC6e_W4iih0-LjodtZz3PGf6JEFNkk2OcSX1rvbvONYOc_tj8Kq34q1SwOFBMKVVoNPivPzZHWhTJoLdySDMxCDZy9NScDWYkq_tfbcB1usKFnCKBqSfEaDaER2QdNtTevCfbXH_NZmS2FAdvYad04jcQwjEgAMo48xSuI38RBwQ4Gjc6xBheYMm03ubzIb7yquaUuubRMfcdGTMXbB2VfpJVh9yP7Y8/file?dl=1\n",
            "Reusing existing connection to uc45a8c7738108004466bab07a95.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81638851 (78M) [application/binary]\n",
            "Saving to: ‘VisDrone/VisDrone2019-DET-val.zip’\n",
            "\n",
            "VisDrone/VisDrone20 100%[===================>]  77.86M  24.4MB/s    in 4.1s    \n",
            "\n",
            "2025-04-27 20:59:41 (19.0 MB/s) - ‘VisDrone/VisDrone2019-DET-val.zip’ saved [81638851/81638851]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def get_image_and_annotation_paths(image_dir, annotation_dir):\n",
        "  # Use image / annotation paths to map to actual training / testing data\n",
        "    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.jpg')))\n",
        "    annotation_paths = [\n",
        "        os.path.join(annotation_dir, os.path.basename(p).replace('.jpg', '.txt'))\n",
        "        for p in image_paths\n",
        "    ]\n",
        "    return image_paths, annotation_paths\n",
        "\n",
        "  # Update paths for your train/val/test sets\n",
        "train_image_paths, train_annotation_paths = get_image_and_annotation_paths(\n",
        "    'VisDrone/VisDrone2019-DET-train/images',\n",
        "    'VisDrone/VisDrone2019-DET-train/annotations'\n",
        ")\n",
        "\n",
        "val_image_paths, val_annotation_paths = get_image_and_annotation_paths(\n",
        "    'VisDrone/VisDrone2019-DET-val/images',\n",
        "    'VisDrone/VisDrone2019-DET-val/annotations'\n",
        ")\n",
        "\n",
        "test_image_paths, test_annotation_paths = get_image_and_annotation_paths(\n",
        "    'VisDrone/VisDrone2019-DET-test-dev/images',\n",
        "    'VisDrone/VisDrone2019-DET-test-dev/annotations'\n",
        ")"
      ],
      "metadata": {
        "id": "Q7AAXi8NxQEP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "\n",
        "Description:\n",
        "\n",
        "The next two blocks of code define a Dataset class, and then utilize that class\n",
        "to create 3 instances, one for the training, validation, and testing splits.\n",
        "\n",
        "We then use those objects to create our Dataloaders, such that our data is ready\n",
        "for use in training our model.\n",
        "\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
      ],
      "metadata": {
        "id": "oZ3MzjDcEi4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisDroneDataset(Dataset):\n",
        "    def __init__(self, image_paths, annotation_paths, resize_to=(640, 640), transforms=None, device='cpu'):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.annotation_paths = annotation_paths\n",
        "\n",
        "        self.resize_to = resize_to\n",
        "        self.transforms = transforms\n",
        "        self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # Load image\n",
        "      img_path = self.image_paths[idx]\n",
        "      img = cv2.imread(img_path)\n",
        "      # Error check to skip faulty image data\n",
        "      if img is None:\n",
        "          print(f\"Failed to load image: {img_path}, skipping.\")\n",
        "          return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      img = cv2.resize(img, self.resize_to)\n",
        "      img = torch.tensor(img / 255.0, dtype=torch.float32).permute(2, 0, 1)\n",
        "\n",
        "      # Load annotation\n",
        "      ann_path = self.annotation_paths[idx]\n",
        "      boxes = []\n",
        "      labels = []\n",
        "\n",
        "      try:\n",
        "          with open(ann_path, 'r') as f:\n",
        "              for line in f:\n",
        "                  line = line.strip()\n",
        "                  if not line:\n",
        "                      continue\n",
        "                  try:\n",
        "                      vals = list(map(int, line.split(',')))\n",
        "                      x, y, w, h, cls_id = vals[0], vals[1], vals[2], vals[3], vals[5]\n",
        "\n",
        "                      # Skip invalid boxes\n",
        "                      if w <= 0 or h <= 0:\n",
        "                          continue\n",
        "\n",
        "                      x2, y2 = x + w, y + h\n",
        "                      if x2 <= x or y2 <= y:\n",
        "                          continue\n",
        "\n",
        "                      # Skip invalid labels\n",
        "                      if cls_id <= 0:\n",
        "                          continue\n",
        "\n",
        "                      boxes.append([x, y, x2, y2])\n",
        "                      labels.append(cls_id - 1)\n",
        "                  except ValueError:\n",
        "                      print(f\"Skipping annotation in: {ann_path}\")\n",
        "                      continue\n",
        "      except FileNotFoundError:\n",
        "          print(f\"Missing annotation file: {ann_path}, skipping.\")\n",
        "          return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "      # Skip samples with no valid annotations\n",
        "      if len(boxes) == 0:\n",
        "          print(f\"No valid boxes in {img_path}, skipping.\")\n",
        "          return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "      boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "      labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      target = {'boxes': boxes, 'labels': labels}\n",
        "      return img, target\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jb58uiH3EWky"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the following lines when training on the full set\n",
        "# train_dataset = VisDroneDataset(train_image_paths, train_annotation_paths, resize_to=(512, 512))\n",
        "# val_dataset = VisDroneDataset(val_image_paths, val_annotation_paths, resize_to=(512, 512))\n",
        "# test_dataset = VisDroneDataset(test_image_paths, test_annotation_paths, resize_to=(512, 512))\n",
        "\n",
        "# For now, using these lines to limit the number of samples trained on, for exploration\n",
        "\n",
        "full_train_dataset = VisDroneDataset(train_image_paths, train_annotation_paths, resize_to=(512, 512), device='cpu')\n",
        "full_val_dataset = VisDroneDataset(val_image_paths, val_annotation_paths, resize_to=(512, 512), device='cpu')\n",
        "full_test_dataset = VisDroneDataset(test_image_paths, test_annotation_paths, resize_to=(512, 512), device='cpu')\n",
        "\n",
        "# Find valid sample indices for training dataset\n",
        "valid_indices = []\n",
        "for i in range(len(full_train_dataset)):\n",
        "    try:\n",
        "        img, target = full_train_dataset[i]\n",
        "        if len(target['boxes']) > 0:\n",
        "            valid_indices.append(i)\n",
        "        # Break early for sample testing\n",
        "        if len(valid_indices) >= 200:\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Use only the first 200 valid entries\n",
        "train_dataset = Subset(full_train_dataset, valid_indices)\n",
        "\n",
        "# Find valid sample indices for validation dataset\n",
        "valid_indices = []\n",
        "for i in range(len(full_val_dataset)):\n",
        "    try:\n",
        "        img, target = full_val_dataset[i]\n",
        "        if len(target['boxes']) > 0:\n",
        "            valid_indices.append(i)\n",
        "        if len(valid_indices) >= 200:\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Use only the first 200 valid entries\n",
        "val_dataset = Subset(full_val_dataset, valid_indices)\n",
        "\n",
        "# Find valid sample indices for validation dataset\n",
        "valid_indices = []\n",
        "for i in range(len(full_test_dataset)):\n",
        "    try:\n",
        "        img, target = full_test_dataset[i]\n",
        "        if len(target['boxes']) > 0:\n",
        "            valid_indices.append(i)\n",
        "        if len(valid_indices) >= 200:\n",
        "            break\n",
        "    except Exception:\n",
        "        continue\n",
        "\n",
        "# Use only the first 200 valid entries\n",
        "test_dataset = Subset(full_test_dataset, valid_indices)\n",
        "\n",
        "# Create dataloaders for each subset\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n",
        "                                   collate_fn=lambda x: tuple(zip(*x)))\n",
        "valid_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda x: tuple(zip(*x)))\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,\n",
        "                                    collate_fn=lambda x: tuple(zip(*x)))"
      ],
      "metadata": {
        "id": "8IBn0BgFFNDw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label map\n",
        "label_map = {\n",
        "    0: 'Ignored',\n",
        "    1: 'Pedestrian',\n",
        "    2: 'Person',\n",
        "    3: 'Car',\n",
        "    4: 'Van',\n",
        "    5: 'Bus',\n",
        "    6: 'Truck',\n",
        "    7: 'Motor',\n",
        "    8: 'Bicycle',\n",
        "    9: 'Awning-tricycle',\n",
        "    10: 'Tricycle',\n",
        "    11: 'Other'\n",
        "}"
      ],
      "metadata": {
        "id": "vnszmsHhyJtl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, TwoMLPHead\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "\n",
        "# Modified TwoMLPHead WITH dropout\n",
        "class TwoMLPHeadWithDropout(TwoMLPHead):\n",
        "    def __init__(self, in_channels, representation_size, dropout_prob=0.5):\n",
        "        super().__init__(in_channels, representation_size)\n",
        "\n",
        "        # Inject dropout after the two FC layers\n",
        "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
        "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.fc6(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc7(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        return x\n",
        "\n",
        "def get_custom_fasterrcnn_model(num_classes, dropout_prob=0.5):\n",
        "    backbone = resnet_fpn_backbone('resnet50', pretrained=True)\n",
        "\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=((16,), (32,), (64,), (128,), (256,)),\n",
        "        aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
        "    )\n",
        "\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0', '1', '2', '3'],\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    model = FasterRCNN(\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler\n",
        "    )\n",
        "\n",
        "    in_features = model.roi_heads.box_head.fc6.in_features\n",
        "    model.roi_heads.box_head = TwoMLPHeadWithDropout(\n",
        "        in_channels=in_features,\n",
        "        representation_size=1024,\n",
        "        dropout_prob=dropout_prob\n",
        "    )\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "w6K95AZRs9Ht"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, num_epochs=50, lr=0.001, device=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(params, lr=lr)\n",
        "\n",
        "    # To store loss history\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "\n",
        "    print(f\"Training started on device: {device}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            images, targets = batch\n",
        "            batch_count += 1\n",
        "\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_dict = model(images, targets)\n",
        "\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += losses.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_loss_history.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad(): # Ensure no gradients are calculated during validation\n",
        "             for batch in valid_loader:\n",
        "                images, targets = batch\n",
        "                batch_count += 1\n",
        "\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss_dict = model(images, targets)\n",
        "\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_val_loss += losses.item()\n",
        "\n",
        "        # --- Epoch End ---\n",
        "        # Print training loss\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    return model, train_loss_history, val_loss_history # Return empty list for val loss history"
      ],
      "metadata": {
        "id": "c9Y3063lq0ZT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train the model\n",
        "def initialize_and_train_model():\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Number of classes in VisDrone dataset (10 classes + background)\n",
        "    num_classes = len(label_map) + 1\n",
        "\n",
        "    # Initialize model\n",
        "    model = get_custom_fasterrcnn_model(num_classes=num_classes, dropout_prob=0.5)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model, train_losses, val_losses = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        valid_loader=valid_loader,\n",
        "        num_epochs=10,\n",
        "        lr=0.001,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return trained_model, train_losses, val_losses"
      ],
      "metadata": {
        "id": "sC3dxzICq4nM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training\n",
        "custom_model, train_loss_history, val_loss_history = initialize_and_train_model()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_loss_history, label='Training Loss')\n",
        "plt.plot(val_loss_history, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model on test set\n",
        "custom_model.eval()\n",
        "map_metric = MeanAveragePrecision()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, targets in tqdm(test_loader, desc=\"Evaluating custom model\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        predictions = custom_model(images)\n",
        "        map_metric.update(predictions, targets)\n",
        "\n",
        "# Compute and print mAP metrics\n",
        "map_results = map_metric.compute()\n",
        "print(f\"Custom Model Results:\")\n",
        "print(f\"mAP (0.50:0.95): {map_results['map']:.8f}\")\n",
        "print(f\"mAP@0.50:        {map_results['map_50']:.8f}\")\n",
        "print(f\"mAP@0.75:        {map_results['map_75']:.8f}\")\n",
        "print(f\"mAP (small):     {map_results['map_small']:.8f}\")\n",
        "print(f\"mAP (medium):    {map_results['map_medium']:.8f}\")\n",
        "print(f\"mAP (large):     {map_results['map_large']:.8f}\")"
      ],
      "metadata": {
        "id": "blVdQKQoq7jO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
