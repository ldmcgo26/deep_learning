{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/ldmcgo26/deep_learning/blob/main/DeepLearning_MS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "id": "4ca4073a0dbda7b6"
  },
  {
   "metadata": {
    "id": "c3f2ead7c38810b"
   },
   "cell_type": "markdown",
   "source": [
    "## Assignment Description\n",
    "\n",
    "Please upload the following materials to the GitHub repository of your project work and submit the URL again:\n",
    "\n",
    "- Python code that downloads, prepares and loads the data (this was the task of Milestone 1, now you have only to adjust it to the other parts of your code)\n",
    "- Python code for the baseline model.\n",
    "- Python code that trains a deep learning model,\n",
    "- Python code that evaluates the results on a (separate) test set,\n",
    "- Updated README.MD file with instructions how to run the code.\n",
    "\n",
    "Please add as much comments to your code as much is needed to be able to easily understand it.\n",
    "\n",
    "At this stage, it is not required to have good (or even reasonable) results, the only requirement is to have the data loading-preparation-training-evaluation pipeline ready."
   ],
   "id": "c3f2ead7c38810b"
  },
  {
   "metadata": {
    "id": "23cce9be47f32da9"
   },
   "cell_type": "markdown",
   "source": [
    "## Our Model Architecture\n",
    "\n",
    "### 1. Transfer Learning:\n",
    "- Load pre-trained ResNet-50 or -101 model with FPN (better suited for drone images)\n",
    "- Freeze or allow fine-tuning\n",
    "- Remove original detection layers (head)\n",
    "\n",
    "### 2. Region Proposal Network (RPN):\n",
    "- Use built-in RPN layers to generate regions\n",
    "- Anchor box tuning as needed (e.g. smaller sizes)\n",
    "\n",
    "### 3. Region of Interest (ROI) Head:\n",
    "- Customize ROI detection box head layers with dropout regularization\n",
    "- Add final box predictor, specified for VisDrone data\n",
    "\n",
    "### 4. Misc. Training Optimizations:\n",
    "- Data augmentation\n",
    "- Optimizers (SGD with momentum, L2 regularization)\n",
    "- Learning rate scheduling\n",
    "- Early stopping"
   ],
   "id": "23cce9be47f32da9"
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "Description:\n",
    "\n",
    "Running the following scripts will collect our VisDrone data and load it into\n",
    "the structure outlined below, primarily utilizing gdown:\n",
    "\n",
    "VisDrone/\n",
    "├── VisDrone2019-DET-test-dev/\n",
    "│   ├── annotations/\n",
    "│   └── images/\n",
    "├── VisDrone2019-DET-train/\n",
    "│   ├── annotations/\n",
    "│   └── images/\n",
    "├── VisDrone2019-DET-val/\n",
    "│   ├── annotations/\n",
    "│   └── images/\n",
    "├── VisDrone2019-DET-test-dev.zip\n",
    "├── VisDrone2019-DET-train.zip\n",
    "└── VisDrone2019-DET-val.zip\n",
    "\n",
    "We then perform several high-level checks of paths and contents to confirm the\n",
    "expected structure above is present in the environment.\n",
    "\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ],
   "metadata": {
    "id": "W79jEkY3zYBM",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "outputId": "d038c244-0d9b-410a-a3cf-d2e5fb5fdd99",
    "ExecuteTime": {
     "end_time": "2025-04-25T00:50:20.931413Z",
     "start_time": "2025-04-25T00:50:20.927224Z"
    }
   },
   "id": "W79jEkY3zYBM",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDescription:\\n\\nRunning the following scripts will collect our VisDrone data and load it into\\nthe structure outlined below, primarily utilizing gdown:\\n\\nVisDrone/\\n├── VisDrone2019-DET-test-dev/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-train/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-val/\\n│   ├── annotations/\\n│   └── images/\\n├── VisDrone2019-DET-test-dev.zip\\n├── VisDrone2019-DET-train.zip\\n└── VisDrone2019-DET-val.zip\\n\\nWe then perform several high-level checks of paths and contents to confirm the\\nexpected structure above is present in the environment.\\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Imports (for version, see requirements.txt)\n",
    "# !pip install torchmetrics\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torchvision.ops import box_iou, MultiScaleRoIAlign\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "import torch.optim as optim\n",
    "import time"
   ],
   "metadata": {
    "id": "h5ZgmXXziOda",
    "ExecuteTime": {
     "end_time": "2025-04-25T01:08:59.073011Z",
     "start_time": "2025-04-25T01:08:59.068509Z"
    }
   },
   "id": "h5ZgmXXziOda",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:35:09.884787Z",
     "start_time": "2025-04-25T01:12:15.939377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "fe9cc6a9463111e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "def get_image_and_annotation_paths(image_dir, annotation_dir):\n",
    "    # Use image / annotation paths to map to actual training / testing data\n",
    "    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.jpg')))\n",
    "    annotation_paths = [\n",
    "        os.path.join(annotation_dir, os.path.basename(p).replace('.jpg', '.txt'))\n",
    "        for p in image_paths\n",
    "    ]\n",
    "    return image_paths, annotation_paths\n",
    "\n",
    "# Update paths for your train/val/test sets with the correct relative paths\n",
    "train_image_paths, train_annotation_paths = get_image_and_annotation_paths(\n",
    "    '../data/VisDrone2019-DET-train/images',\n",
    "    '../data/VisDrone2019-DET-train/annotations'\n",
    ")\n",
    "\n",
    "val_image_paths, val_annotation_paths = get_image_and_annotation_paths(\n",
    "    '../data/VisDrone2019-DET-val/images',\n",
    "    '../data/VisDrone2019-DET-val/annotations'\n",
    ")\n",
    "\n",
    "test_image_paths, test_annotation_paths = get_image_and_annotation_paths(\n",
    "    '../data/VisDrone2019-DET-test-dev/images',\n",
    "    '../data/VisDrone2019-DET-test-dev/annotations'\n",
    ")"
   ],
   "metadata": {
    "id": "Q7AAXi8NxQEP",
    "ExecuteTime": {
     "end_time": "2025-04-25T01:09:10.043042Z",
     "start_time": "2025-04-25T01:09:09.979089Z"
    }
   },
   "id": "Q7AAXi8NxQEP",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T00:56:01.265522Z",
     "start_time": "2025-04-25T00:56:01.223850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to check if a directory exists and count files\n",
    "def check_directory(path, file_extension=None):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"❌ Path does not exist: {path}\")\n",
    "        return False\n",
    "\n",
    "    if file_extension:\n",
    "        files = [f for f in os.listdir(path) if f.endswith(file_extension)]\n",
    "        print(f\"✅ Path exists: {path} (contains {len(files)} {file_extension} files)\")\n",
    "    else:\n",
    "        files = os.listdir(path)\n",
    "        print(f\"✅ Path exists: {path} (contains {len(files)} files)\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Check all directories\n",
    "print(\"Checking training directories:\")\n",
    "train_img_dir = '../data/VisDrone2019-DET-train/images'\n",
    "train_ann_dir = '../data/VisDrone2019-DET-train/annotations'\n",
    "check_directory(train_img_dir, '.jpg')\n",
    "check_directory(train_ann_dir, '.txt')\n",
    "\n",
    "print(\"\\nChecking validation directories:\")\n",
    "val_img_dir = '../data/VisDrone2019-DET-val/images'\n",
    "val_ann_dir = '../data/VisDrone2019-DET-val/annotations'\n",
    "check_directory(val_img_dir, '.jpg')\n",
    "check_directory(val_ann_dir, '.txt')\n",
    "\n",
    "print(\"\\nChecking test directories:\")\n",
    "test_img_dir = '../data/VisDrone2019-DET-test-dev/images'\n",
    "test_ann_dir = '../data/VisDrone2019-DET-test-dev/annotations'\n",
    "check_directory(test_img_dir, '.jpg')\n",
    "check_directory(test_ann_dir, '.txt')\n",
    "\n",
    "# Check if the paths match by comparing file counts\n",
    "print(\"\\nVerifying image and annotation counts match:\")\n",
    "if len(train_image_paths) == len(train_annotation_paths):\n",
    "    print(f\"Training: {len(train_image_paths)} images and annotations match\")\n",
    "else:\n",
    "    print(f\"X Training: {len(train_image_paths)} images but {len(train_annotation_paths)} annotations\")\n",
    "\n",
    "if len(val_image_paths) == len(val_annotation_paths):\n",
    "    print(f\"Validation: {len(val_image_paths)} images and annotations match\")\n",
    "else:\n",
    "    print(f\"X Validation: {len(val_image_paths)} images but {len(val_annotation_paths)} annotations\")\n",
    "\n",
    "if len(test_image_paths) == len(test_annotation_paths):\n",
    "    print(f\"Testing: {len(test_image_paths)} images and annotations match\")\n",
    "else:\n",
    "    print(f\"X Testing: {len(test_image_paths)} images but {len(test_annotation_paths)} annotations\")\n",
    "\n",
    "# Try to access a sample file from each directory (if they exist)\n",
    "print(\"\\nTrying to access sample files:\")\n",
    "if train_image_paths and train_annotation_paths:\n",
    "    print(f\"Sample training image path: {train_image_paths[0]}\")\n",
    "    print(f\"Sample training annotation path: {train_annotation_paths[0]}\")\n",
    "\n",
    "    # Check if the sample files exist\n",
    "    if os.path.exists(train_image_paths[0]) and os.path.exists(train_annotation_paths[0]):\n",
    "        print(\"Sample training files exist\")\n",
    "    else:\n",
    "        print(\"X Sample training files don't exist\")"
   ],
   "id": "4981e42e4010f32b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training directories:\n",
      "✅ Path exists: ../data/VisDrone2019-DET-train/images (contains 6471 .jpg files)\n",
      "✅ Path exists: ../data/VisDrone2019-DET-train/annotations (contains 6471 .txt files)\n",
      "\n",
      "Checking validation directories:\n",
      "✅ Path exists: ../data/VisDrone2019-DET-val/images (contains 548 .jpg files)\n",
      "✅ Path exists: ../data/VisDrone2019-DET-val/annotations (contains 548 .txt files)\n",
      "\n",
      "Checking test directories:\n",
      "✅ Path exists: ../data/VisDrone2019-DET-test-dev/images (contains 1610 .jpg files)\n",
      "✅ Path exists: ../data/VisDrone2019-DET-test-dev/annotations (contains 1610 .txt files)\n",
      "\n",
      "Verifying image and annotation counts match:\n",
      "Training: 6471 images and annotations match\n",
      "Validation: 548 images and annotations match\n",
      "Testing: 1610 images and annotations match\n",
      "\n",
      "Trying to access sample files:\n",
      "Sample training image path: ../data/VisDrone2019-DET-train/images/0000002_00005_d_0000014.jpg\n",
      "Sample training annotation path: ../data/VisDrone2019-DET-train/annotations/0000002_00005_d_0000014.txt\n",
      "Sample training files exist\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:09:15.884359Z",
     "start_time": "2025-04-25T01:09:15.879106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "Description:\n",
    "\n",
    "The next two blocks of code define a Dataset class, and then utilize that class\n",
    "to create 3 instances, one for the training, validation, and testing splits.\n",
    "\n",
    "We then use those objects to create our Dataloaders, such that our data is ready\n",
    "for use in training our model.\n",
    "\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ],
   "id": "oZ3MzjDcEi4s",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nDescription:\\n\\nThe next two blocks of code define a Dataset class, and then utilize that class\\nto create 3 instances, one for the training, validation, and testing splits.\\n\\nWe then use those objects to create our Dataloaders, such that our data is ready\\nfor use in training our model.\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:18:04.550576Z",
     "start_time": "2025-04-25T01:18:04.541365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotation_paths, resize_to=(640, 640), transforms=None, device='mps'):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.annotation_paths = annotation_paths\n",
    "\n",
    "        self.resize_to = resize_to\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      # Load image\n",
    "      img_path = self.image_paths[idx]\n",
    "      img = cv2.imread(img_path)\n",
    "      # Error check to skip faulty image data\n",
    "      if img is None:\n",
    "          print(f\"Failed to load image: {img_path}, skipping.\")\n",
    "          return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "      img = cv2.resize(img, self.resize_to)\n",
    "      img = torch.tensor(img / 255.0, dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "      # Load annotation\n",
    "      ann_path = self.annotation_paths[idx]\n",
    "      boxes = []\n",
    "      labels = []\n",
    "\n",
    "      try:\n",
    "          with open(ann_path, 'r') as f:\n",
    "              for line in f:\n",
    "                  line = line.strip()\n",
    "                  if not line:\n",
    "                      continue\n",
    "                  try:\n",
    "                      vals = list(map(int, line.split(',')))\n",
    "                      x, y, w, h, cls_id = vals[0], vals[1], vals[2], vals[3], vals[5]\n",
    "\n",
    "                      # Skip invalid boxes\n",
    "                      if w <= 0 or h <= 0:\n",
    "                          continue\n",
    "\n",
    "                      x2, y2 = x + w, y + h\n",
    "                      if x2 <= x or y2 <= y:\n",
    "                          continue\n",
    "\n",
    "                      # Skip invalid labels\n",
    "                      if cls_id <= 0:\n",
    "                          continue\n",
    "\n",
    "                      boxes.append([x, y, x2, y2])\n",
    "                      labels.append(cls_id)\n",
    "                  except ValueError:\n",
    "                      print(f\"Skipping annotation in: {ann_path}\")\n",
    "                      continue\n",
    "      except FileNotFoundError:\n",
    "          print(f\"Missing annotation file: {ann_path}, skipping.\")\n",
    "          return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "      # Skip samples with no valid annotations\n",
    "      if len(boxes) == 0:\n",
    "          print(f\"No valid boxes in {img_path}, skipping.\")\n",
    "          return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "      boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "      labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "      target = {'boxes': boxes, 'labels': labels}\n",
    "      return img, target"
   ],
   "id": "1d076156dfe4e809",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:18:10.310575Z",
     "start_time": "2025-04-25T01:18:07.708361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use the following lines when training on the full set\n",
    "# train_dataset = VisDroneDataset(train_image_paths, train_annotation_paths, resize_to=(512, 512))\n",
    "# val_dataset = VisDroneDataset(val_image_paths, val_annotation_paths, resize_to=(512, 512))\n",
    "# test_dataset = VisDroneDataset(test_image_paths, test_annotation_paths, resize_to=(512, 512))\n",
    "\n",
    "# For now, using these lines to limit the number of samples trained on, for exploration\n",
    "\n",
    "full_train_dataset = VisDroneDataset(train_image_paths, train_annotation_paths, resize_to=(512, 512), device='cpu')\n",
    "full_val_dataset = VisDroneDataset(val_image_paths, val_annotation_paths, resize_to=(512, 512), device='cpu')\n",
    "full_test_dataset = VisDroneDataset(test_image_paths, test_annotation_paths, resize_to=(512, 512), device='cpu')\n",
    "\n",
    "# Find valid sample indices for training dataset\n",
    "valid_indices = []\n",
    "for i in range(len(full_train_dataset)):\n",
    "    try:\n",
    "        img, target = full_train_dataset[i]\n",
    "        if len(target['boxes']) > 0:\n",
    "            valid_indices.append(i)\n",
    "        # Break early for sample testing\n",
    "        if len(valid_indices) >= 200:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Use only the first 200 valid entries\n",
    "train_dataset = Subset(full_train_dataset, valid_indices)\n",
    "\n",
    "# Find valid sample indices for validation dataset\n",
    "valid_indices = []\n",
    "for i in range(len(full_val_dataset)):\n",
    "    try:\n",
    "        img, target = full_val_dataset[i]\n",
    "        if len(target['boxes']) > 0:\n",
    "            valid_indices.append(i)\n",
    "        if len(valid_indices) >= 200:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Use only the first 200 valid entries\n",
    "val_dataset = Subset(full_val_dataset, valid_indices)\n",
    "\n",
    "# Find valid sample indices for validation dataset\n",
    "valid_indices = []\n",
    "for i in range(len(full_test_dataset)):\n",
    "    try:\n",
    "        img, target = full_test_dataset[i]\n",
    "        if len(target['boxes']) > 0:\n",
    "            valid_indices.append(i)\n",
    "        if len(valid_indices) >= 200:\n",
    "            break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "# Use only the first 200 valid entries\n",
    "test_dataset = Subset(full_test_dataset, valid_indices)\n",
    "\n",
    "# Create dataloaders for each subset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,\n",
    "                                   collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,\n",
    "                                    collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,\n",
    "                                    collate_fn=lambda x: tuple(zip(*x)))"
   ],
   "id": "53051cac029b580a",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:16:32.352806Z",
     "start_time": "2025-04-25T01:16:32.349338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# VisDrone Label map\n",
    "label_map = {\n",
    "    0: 'Ignored',\n",
    "    1: 'Pedestrian',\n",
    "    2: 'Person',\n",
    "    3: 'Car',\n",
    "    4: 'Van',\n",
    "    5: 'Bus',\n",
    "    6: 'Truck',\n",
    "    7: 'Motor',\n",
    "    8: 'Bicycle',\n",
    "    9: 'Awning-tricycle',\n",
    "    10: 'Tricycle',\n",
    "    11: 'Other'\n",
    "}"
   ],
   "id": "c6786fc3e020e68f",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:18:12.791296Z",
     "start_time": "2025-04-25T01:18:12.351865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# BASELINE MODEL:\n",
    "\n",
    "# Load a pre-trained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Replace the classifier with one that matches your dataset\n",
    "num_classes = len(label_map) + 1  # +1 for background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"mps\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Reduce number of region proposals to improve speed\n",
    "model.rpn.pre_nms_top_n_train = 1000\n",
    "model.rpn.post_nms_top_n_train = 300\n",
    "model.rpn.pre_nms_top_n_test = 500\n",
    "model.rpn.post_nms_top_n_test = 100"
   ],
   "id": "2cd18a9e7234bdd6",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T01:14:09.081799Z",
     "start_time": "2025-04-25T01:14:09.078401Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.mps.is_available())",
   "id": "c4d70ff68ab929ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-25T01:18:15.173259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the baseline model\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # Record epoch loss\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} loss: {epoch_loss:.4f}\")\n",
    "    lr_scheduler.step()\n",
    "\n",
    "# Validation pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        outputs = model(images)\n",
    "        break"
   ],
   "id": "84f30ffe6b1d9e2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:  20%|██        | 10/50 [01:30<06:08,  9.21s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Function to visualize the output predicitons made by the model\n",
    "def show_prediction(img_tensor, pred, label_map=None, score_thresh=0.5):\n",
    "    img = img_tensor.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    boxes = pred['boxes'].cpu()\n",
    "    labels = pred['labels'].cpu()\n",
    "    scores = pred['scores'].cpu()\n",
    "\n",
    "    # Print prediction info\n",
    "    print(\"Prediction Summary:\")\n",
    "    print(f\"  Total predictions: {len(boxes)}\")\n",
    "    print(f\"  Scores: {scores}\")\n",
    "    print(f\"  Boxes: {boxes}\")\n",
    "    print(f\"  Labels: {labels}\")\n",
    "\n",
    "    has_detections = False\n",
    "\n",
    "    # Visualize each bounding box using patches package and plt\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score < score_thresh:\n",
    "            continue\n",
    "        has_detections = True\n",
    "        x1, y1, x2, y2 = box\n",
    "        width, height = x2 - x1, y2 - y1\n",
    "        rect = patches.Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        label_str = f\"{label}\"\n",
    "        if label_map and label in label_map:\n",
    "            label_str = label_map[label]\n",
    "        ax.text(x1, y1 - 5, f\"{label_str} ({score:.2f})\", color='yellow', fontsize=12)\n",
    "\n",
    "    if not has_detections:\n",
    "        print(f\"No predictions above threshold ({score_thresh})\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "id": "448d113307301b62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "show_prediction(images[0], outputs[0], label_map=label_map, score_thresh=0.05)",
   "id": "2c27b8b58a3faabc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model Evaluation:\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "iou_threshold=0.5\n",
    "confidence_threshold=0.5\n",
    "\n",
    "# Initialize mAP metric from torchmetrics\n",
    "map_metric = MeanAveragePrecision()\n",
    "\n",
    "all_preds = []\n",
    "all_gts = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        # Move data to device\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(images)\n",
    "\n",
    "        # Update mAP metric\n",
    "        map_metric.update(preds, targets)\n",
    "\n",
    "        # Collect predictions and ground truth labels for precision/recall/F1\n",
    "        for pred, tgt in zip(preds, targets):\n",
    "            pred_boxes = pred['boxes']\n",
    "            pred_labels = pred['labels']\n",
    "            pred_scores = pred['scores']\n",
    "\n",
    "            gt_boxes = tgt['boxes']\n",
    "            gt_labels = tgt['labels']\n",
    "\n",
    "            # Filter predictions based on confidence score\n",
    "            valid_preds = pred_scores > confidence_threshold\n",
    "            pred_boxes = pred_boxes[valid_preds]\n",
    "            pred_labels = pred_labels[valid_preds]\n",
    "\n",
    "            # Match predictions to ground truth using IoU threshold\n",
    "            ious = box_iou(pred_boxes, gt_boxes)\n",
    "            matched_gt_indices = torch.argmax(ious, dim=1)\n",
    "\n",
    "            # Only keep predictions that have at least one match with ground truth\n",
    "            for i, pred_label in enumerate(pred_labels):\n",
    "                if ious[i, matched_gt_indices[i]] > iou_threshold:\n",
    "                    all_preds.append(pred_label.item())\n",
    "                    all_gts.append(gt_labels[matched_gt_indices[i]].item())\n",
    "\n",
    "# Compute mAP\n",
    "map_metrics = map_metric.compute()"
   ],
   "id": "cfc89f44f58732ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"mAP (0.50:0.95): {map_metrics['map']:.8f}\")\n",
    "print(f\"mAP@0.50:        {map_metrics['map_50']:.8f}\")\n",
    "print(f\"mAP@0.75:        {map_metrics['map_75']:.8f}\")\n",
    "print(f\"mAP (small):     {map_metrics['map_small']:.8f}\")\n",
    "print(f\"mAP (medium):    {map_metrics['map_medium']:.8f}\")\n",
    "print(f\"mAP (large):     {map_metrics['map_large']:.8f}\")\n",
    "\n",
    "# Compute Precision, Recall, and F1 score for each class\n",
    "classes = list(range(1, 13))\n",
    "precision = precision_score(all_gts, all_preds, average=None, labels=classes)\n",
    "recall = recall_score(all_gts, all_preds, average=None, labels=classes)\n",
    "f1 = f1_score(all_gts, all_preds, average=None, labels=classes)\n",
    "\n",
    "# Print out precision, recall, and F1 per class\n",
    "for cls, p, r, f in zip(classes, precision, recall, f1):\n",
    "    print(f\"Class {cls:2d} → Precision: {p:.8f}, Recall: {r:.8f}, F1: {f:.8f}\")"
   ],
   "id": "496006a7cc91e57d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Custom model MLP head with dropout regularization\n",
    "class TwoMLPHeadWithDropout(nn.Module):\n",
    "    \"\"\"A two-layer MLP head with dropout for ROI features.\"\"\"\n",
    "    def __init__(self, in_channels, representation_size=1024, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    "        self.fc6 = nn.Linear(in_channels, representation_size)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.dropout6 = nn.Dropout(p=dropout_prob)\n",
    "        self.fc7 = nn.Linear(representation_size, representation_size)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.dropout7 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is [N, C, H, W] → flatten\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc6(x)\n",
    "        x = self.relu6(x)\n",
    "        x = self.dropout6(x)\n",
    "        x = self.fc7(x)\n",
    "        x = self.relu7(x)\n",
    "        x = self.dropout7(x)  # Apply dropout after second layer too\n",
    "        return x"
   ],
   "id": "daa6ffa414718408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Custom Faster RCNN model with pretrained backbone, custom RPN, ROI head\n",
    "def get_custom_model(\n",
    "    num_classes,\n",
    "    backbone_name='resnet50',  # optionally 'resnet101'\n",
    "    trainable_backbone_layers=3,\n",
    "    dropout_prob=0.5,\n",
    "    # manually set smaller anchor sizes\n",
    "    anchor_sizes=((16,), (32,), (64,), (128,), (256,)),\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    "):\n",
    "\n",
    "    # 1) Backbone + FPN (Feature Pyramid Network)\n",
    "    backbone = resnet_fpn_backbone(\n",
    "        backbone_name,\n",
    "        pretrained=True,\n",
    "        trainable_layers=trainable_backbone_layers\n",
    "    )\n",
    "\n",
    "    # 2) RPN with custom anchor generator (tuned for drone imagery)\n",
    "    rpn_anchor_generator = AnchorGenerator(\n",
    "        sizes=anchor_sizes,\n",
    "        aspect_ratios=aspect_ratios\n",
    "    )\n",
    "\n",
    "    # 3) ROI Align for feature extraction\n",
    "    roi_pooler = MultiScaleRoIAlign(\n",
    "        featmap_names=['0', '1', '2', '3'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    # 4) Build the Faster R-CNN model\n",
    "    model = FasterRCNN(\n",
    "        backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=rpn_anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        # Tune RPN parameters\n",
    "        rpn_pre_nms_top_n_train=2000,\n",
    "        rpn_post_nms_top_n_train=1000,\n",
    "        rpn_pre_nms_top_n_test=1000,\n",
    "        rpn_post_nms_top_n_test=500,\n",
    "        rpn_nms_thresh=0.7,\n",
    "        rpn_fg_iou_thresh=0.7,\n",
    "        rpn_bg_iou_thresh=0.3,\n",
    "        # ROI parameters\n",
    "        box_score_thresh=0.05,\n",
    "        box_nms_thresh=0.5,\n",
    "        box_detections_per_img=100,\n",
    "        box_fg_iou_thresh=0.5,\n",
    "        box_bg_iou_thresh=0.5\n",
    "    )\n",
    "\n",
    "    # 5) Replace box_head with our custom dropout-regularized MLP\n",
    "    in_channels = model.roi_heads.box_head.fc6.in_features\n",
    "    model.roi_heads.box_head = TwoMLPHeadWithDropout(\n",
    "        in_channels,\n",
    "        representation_size=1024,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "\n",
    "    # 6) Replace the box predictor for our specific number of classes\n",
    "    feat_in = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(feat_in, num_classes)\n",
    "\n",
    "    return model"
   ],
   "id": "463ffc3d5ef64377"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training function with optimizations SGD, learning rate scheduling, early stopping\n",
    "def train_model(model, train_loader, valid_loader, num_epochs=10, lr=0.005, device='mps'):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Optimizer: SGD with momentum and L2 regularization\n",
    "    optimizer = optim.SGD(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0005  # L2 regularization\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=3,\n",
    "        gamma=0.1\n",
    "    )\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 3\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "\n",
    "    # Track loss history\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            # Move data to device\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            epoch_loss += losses.item()\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in valid_loader:\n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += losses.item()\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "\n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {avg_train_loss:.4f}, '\n",
    "              f'Val Loss: {avg_val_loss:.4f}, '\n",
    "              f'Time: {time_elapsed:.2f}s')\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            if early_stop_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model, train_loss_history, val_loss_history"
   ],
   "id": "680f21120d9e4b38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize and train the model\n",
    "def initialize_and_train_model():\n",
    "    # Set device\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    # Number of classes in VisDrone dataset (10 classes + background)\n",
    "    num_classes = len(label_map) + 1\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_custom_model(num_classes=num_classes)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model, train_losses, val_losses = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        num_epochs=10,\n",
    "        lr=0.001,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    return trained_model, train_losses, val_losses"
   ],
   "id": "29c35c26b856ed2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run the training\n",
    "custom_model, train_loss_history, val_loss_history = initialize_and_train_model()\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(val_loss_history, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on test set\n",
    "custom_model.eval()\n",
    "map_metric = MeanAveragePrecision()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_loader, desc=\"Evaluating custom model\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        predictions = custom_model(images)\n",
    "        map_metric.update(predictions, targets)\n",
    "\n",
    "# Compute and print mAP metrics\n",
    "map_results = map_metric.compute()\n",
    "print(f\"Custom Model Results:\")\n",
    "print(f\"mAP (0.50:0.95): {map_results['map']:.8f}\")\n",
    "print(f\"mAP@0.50:        {map_results['map_50']:.8f}\")\n",
    "print(f\"mAP@0.75:        {map_results['map_75']:.8f}\")\n",
    "print(f\"mAP (small):     {map_results['map_small']:.8f}\")\n",
    "print(f\"mAP (medium):    {map_results['map_medium']:.8f}\")\n",
    "print(f\"mAP (large):     {map_results['map_large']:.8f}\")"
   ],
   "id": "9c9e854df25ff523"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
